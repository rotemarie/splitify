# Splitify - AI Music Source Separation

Splitify is an end-to-end system that separates music into 4 stems: **vocals**, **bass**, **drums**, and **other** (melody/instruments) using deep learning.

## ğŸµ Features

- **Multi-stem separation**: Separate any song into 4 distinct tracks
- **End-to-end pipeline**: From raw audio to separated stems
- **Flexible training**: Train on MUSDB18 or custom datasets
- **CPU/GPU support**: Works on both CPU and GPU
- **Multiple formats**: Supports MP3, WAV, FLAC, and more

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd Splitify

# Install dependencies
pip install -r requirements.txt
```

### 2. Download a Pre-trained Model (Coming Soon)
```bash
# We'll provide pre-trained models in the future
# For now, you'll need to train your own model
```

### 3. Separate Your Music

```bash
python inference.py \
    --model_path checkpoints/your_model.pt \
    --input "path/to/your/song.mp3" \
    --output_dir "separated_stems/"
```

This will create 4 files:
- `song_vocals.wav` - Isolated vocals
- `song_bass.wav` - Bass line
- `song_drums.wav` - Drum track  
- `song_other.wav` - Other instruments/melody

## ğŸ‹ï¸ Training Your Own Model

### 1. Prepare MUSDB18 Dataset

```bash
# Download MUSDB18 dataset to musdb18/ folder
# Update config.json with your paths

# Convert to HDF5 format for fast training
python preprocessing/audio_to_hdf5.py --config config.json

# Create training indexes
python preprocessing/index.py \
    --workspace /path/to/Splitify \
    --config_yaml preprocessing/configs/sr=44100,vocals-bass-drums-other.yaml \
    --split train
```

### 2. Train the Model

```bash
cd learning/multistem

python train.py \
    --workspace ../../checkpoints \
    --index_pkl ../../indexes/musdb18/train/sr=44100,vocals-bass-drums-other.pkl \
    --epochs 50 \
    --batch_size 8 \
    --lr 1e-3
```

### 3. Evaluate Your Model

```bash
python evaluate_multistem.py \
    --model_path ../../checkpoints/checkpoint_epoch50.pt \
    --input_audio "test_song.wav" \
    --output_dir "results/"
```

## ğŸ–¥ï¸ CPU vs GPU Training

### GPU Training (Recommended)
- **Training time**: ~6-12 hours for 50 epochs on RTX 3080
- **Memory needed**: 8GB+ VRAM
- **Batch size**: 8-16

### CPU Training (Possible but Slow)
- **Training time**: ~3-7 days for 50 epochs
- **Memory needed**: 16GB+ RAM
- **Batch size**: 2-4 (reduce to avoid memory issues)
- **Recommendation**: Use smaller model (`base_channels=32` instead of 64)

To train on CPU:
```bash
python train.py \
    --workspace ../../checkpoints \
    --index_pkl ../../indexes/musdb18/train/sr=44100,vocals-bass-drums-other.pkl \
    --epochs 50 \
    --batch_size 2 \
    --lr 1e-3
```

## ğŸ“ Project Structure

```
Splitify/
â”œâ”€â”€ inference.py              # Main inference script
â”œâ”€â”€ config.json              # Dataset configuration
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ preprocessing/           # Data preparation
â”‚   â”œâ”€â”€ audio_to_hdf5.py    # Convert MUSDB18 to HDF5
â”‚   â”œâ”€â”€ index.py            # Create training indexes
â”‚   â””â”€â”€ configs/            # Configuration files
â”œâ”€â”€ learning/
â”‚   â”œâ”€â”€ multistem/          # Multi-stem separation
â”‚   â”‚   â”œâ”€â”€ model.py        # U-Net architecture
â”‚   â”‚   â”œâ”€â”€ train.py        # Training script
â”‚   â”‚   â”œâ”€â”€ dataset.py      # Data loading
â”‚   â”‚   â””â”€â”€ stft_utils.py   # STFT/iSTFT utilities
â”‚   â””â”€â”€ singlestem/         # Single-stem separation
â”œâ”€â”€ musdb18/                # MUSDB18 dataset (you provide)
â”œâ”€â”€ hdf5s/                  # Processed HDF5 files
â”œâ”€â”€ indexes/                # Training indexes
â””â”€â”€ checkpoints/            # Saved models
```

## ğŸ¯ Model Architecture

The system uses a **U-Net** architecture that:

1. **Input**: Takes STFT magnitude spectrogram of mixed audio
2. **Processing**: Uses encoder-decoder with skip connections
3. **Output**: Predicts 4 masks (one per stem) 
4. **Reconstruction**: Applies masks to original spectrogram and converts back to audio

## ğŸ”§ Customization

### Change Stems
Edit the stems in your training script:
```python
stems = ["vocals", "piano", "guitar", "drums"]  # Custom stems
```

### Adjust Model Size
For faster training or CPU use:
```python
model = UNetMulti(in_channels=1, n_outputs=4, base_channels=32)  # Smaller model
```

### Different Audio Parameters
Modify STFT parameters in training:
```python
n_fft = 1024        # Smaller = faster, less frequency resolution
hop_length = 256    # Smaller = more time resolution, slower
```

## ğŸ“Š Expected Results

With proper training on MUSDB18:
- **Vocals**: Clean separation, minimal bleeding
- **Drums**: Good transient preservation
- **Bass**: Clear low-frequency separation  
- **Other**: Mixed results depending on complexity

## ğŸ› Troubleshooting

### Out of Memory
- Reduce batch size: `--batch_size 2`
- Use smaller model: `base_channels=32`
- Process shorter segments

### Poor Separation Quality
- Train longer: `--epochs 100`
- Use larger model: `base_channels=128`
- Ensure good quality training data
- Check your STFT parameters

### Slow Training
- Use GPU if available
- Increase batch size if memory allows
- Use mixed precision training (advanced)

## ğŸ“ License

[Add your license here]

## ğŸ¤ Contributing

[Add contribution guidelines]

## ğŸ“š References

- MUSDB18 Dataset
- U-Net: Convolutional Networks for Biomedical Image Segmentation
- Open-Unmix: A Reference Implementation for Music Source Separation
